{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyM7GxiCVtUTA1fDYiXs+Wgp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UdWE5aPlYGuj","executionInfo":{"status":"ok","timestamp":1666063807778,"user_tz":420,"elapsed":2145,"user":{"displayName":"Archana Kannan","userId":"16237662099459153058"}},"outputId":"55af78f4-db47-40f3-bdbe-b56610c6ecf7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["# Mounting Google drive used for this code.\n","# The inputs and outputs to the code are stored in the Google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Import files\n","import numpy as np\n","from PIL import Image\n","import datetime\n","import pandas as pd\n","import tensorflow as tf\n","import math\n","import statistics\n","from scipy.stats.stats import pearsonr\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","import random\n","import datetime"]},{"cell_type":"code","source":["# Paths for data and outputs\n","path_PPT = '/content/gdrive/My Drive/Mixil_work/D-SHIELD/Codes/Inputs/SMAP_PPT_global/'\n","path_SM = '/content/gdrive/My Drive/Mixil_work/D-SHIELD/Codes/Inputs/SMAP_SM_global_ass_Oct/'\n","path_OP = \"/content/gdrive/My Drive/Mixil_work/D-SHIELD/Codes/Outputs/Uncertainty_variation_exp_v2/4_Oct/run6/\"\n","\n","# Lists to store RMSE and bias calculated agaisnt the true data (SMAP L4)\n","RMSE = [] \n","bias = []\n","\n","# The dates can be changed, this is coded to take the previous three I/P files from \"start\" datetime to forecast till \"end\" datetime\n","for i_date in range(4,6):\n","  start = datetime.datetime(2020,10,i_date,16,30,0) # START DATETIME\n","  end = datetime.datetime(2020,10,i_date+3,22,30,0) # END DATETIME\n","  delta = datetime.timedelta(hours=3) # DELTA T. For SMAP L4 DELTA=3hours\n","\n","  frames_PPT = [] \n","  frames_SM = []\n","\n","  # while loop to collect all the SM and PPT data for the specified dates.\n","  # Note that in real-time running, only the antecedent I/Ps are required.\n","  # To compare with actual SMAP, all the dates are collected.\n","  while start <= end:\n","    d = start.strftime(\"%Y%m%d%H%M%S\")\n","    img_PPT = Image.open(path_PPT+d+\"_ppt.jpeg\")# Importing SMAP precipitation data\n","\n","    # Image preprocessing\n","    f_PPT = img_PPT.convert(mode='RGB')\n","    f_PPT = np.array(f_PPT)\n","    f_PPT = np.sum(f_PPT, axis=2)\n","    f_PPT = np.array(f_PPT)\n","    f_PPT = f_PPT.reshape(f_PPT.shape[0], f_PPT.shape[1], 1)\n","    frames_PPT.append(f_PPT)\n","    \n","    img_SM = Image.open(path_SM + d +\"_sm.jpeg\")# Importing SMAP soil moisture data\n","\n","    # Image preprocessing\n","    f_SM = img_SM.convert(mode='RGB')\n","    f_SM = np.array(f_SM)\n","    f_SM = np.sum(f_SM, axis=2)\n","    f_SM = np.array(f_SM)\n","    f_SM = f_SM.reshape(f_SM.shape[0], f_SM.shape[1], 1)\n","    frames_SM.append(f_SM)\n","\n","    start += delta\n","\n","  frames_PPT = np.array(frames_PPT)\n","  frames_SM = np.array(frames_SM)\n","  print(f\"Shape of frames_PPT is {np.shape(frames_PPT)}\")\n","  print(f\"shape of frames_SM is {np.shape(frames_SM)}\")\n","\n","  # Max-Min Normalization of the SM and PPT images\n","\n","  test_frames_SM = frames_SM[0:(len(frames_SM)),:,:,:]\n","  test_frames_PPT = frames_PPT[0:(len(frames_PPT)),:,:,:]\n","\n","  maxi_SM = np.max(test_frames_SM)\n","  mini_SM = np.min(test_frames_SM)\n","\n","  maxi_PPT = np.max(test_frames_PPT)\n","  mini_PPT = np.min(test_frames_PPT)\n","\n","  test_frames_SM = (test_frames_SM-mini_SM)/(maxi_SM-mini_SM)\n","  test_frames_PPT = (test_frames_PPT-mini_PPT)/(maxi_PPT-mini_PPT)\n","\n","  print(f\"Shape of test_frames_SM is {np.shape(test_frames_SM)}\")\n","  print(f\"Shape of test_frames_PPT is {np.shape(test_frames_PPT)}\")\n","\n","  window = 3 # Interval, using 9 hours interval(i.e., three data prediction)\n","\n","  # The window size is a variable. Slight modifications to the code is required if window size is changed\n","\n","  # Appending 3 images to a single variable ---> \"input to the model\" data structure\n","  X_test_SM, X_test_PPT, y_act = [], [], []\n","\n","  for i in range(0, len(test_frames_PPT), 3):\n","    X_test_SM.append(test_frames_SM[i:i+window,::,::,::])\n","    X_test_PPT.append(test_frames_PPT[i:i+window,::,::,::])\n","\n","  for i in range(0, len(test_frames_SM), 3):\n","    y_act.append(test_frames_SM[i:i+window,::,::,::])\n","    \n","  X_test_SM = np.array(X_test_SM)\n","  X_test_PPT = np.array(X_test_PPT)\n","  y_act = np.array(y_act)\n","\n","  print(f\"Shape of X_test_SM is {np.shape(X_test_SM)}\")\n","  print(f\"Shape of X_test_PPT is {np.shape(X_test_PPT)}\")\n","  print(f\"Shape of y_act is {np.shape(y_act)}\")\n","\n","  # Loading trained model\n","\n","  model = tf.keras.models.load_model('/content/gdrive/My Drive/Mixil_work/D-SHIELD/Codes/Models/global_model_v1.h5')\n","\n","  # Prediction\n","\n","  # Lists to store intermediate average RMSE and bias of the predictions\n","  avg_RMSE = []\n","  avg_bias = []\n","  \n","  # a and b are just dummy variables, since np.newaxis did not work in a single line\n","  # xi's are the inputs, other inputs can be added\n","\n","  a = X_test_SM[0,::,::,::,::] \n","  x1 = a[np.newaxis,::,::,::,::] # x1 is the input - SM\n","\n","  dfs = []\n","\n","  # Predicting for three days\n","  for i in range(1,9):\n","    b = X_test_PPT[i-1,::,::,::,::]\n","    x2 = b[np.newaxis,::,::,::,::] # x2 is the input - PPT\n","    \n","    # Predicting in three hour intervals, model trained that way\n","    for j in range(0,3):\n","      predictions = []\n","      n_iter = 2 # Number of iterations for prediction, using dropout layer to predict the mean and other statistics\n","      for t in range(n_iter):\n","        pred = model.predict([x1, x2])\n","        y_pred = pred[::,j,::,::,::].flatten()\n","        predictions.append(y_pred)\n","      \n","      prediction_df = pd.DataFrame()\n","      pred_array = np.array(predictions)\n","      prediction_df['mean'] = pred_array.mean(axis=0).reshape(-1,)\n","      prediction_df['std'] = pred_array.std(axis=0).reshape(-1,)\n","      prediction_df['var'] = pred_array.var(axis=0).reshape(-1,)\n","\n","      y_true = y_act[i,j,::,::,::].flatten() # Flattened image as CSV\n","\n","      pred_mean = prediction_df['mean']\n","      std = prediction_df['std']\n","      var = prediction_df['var']\n","\n","      dict_tosave = {\"y_pred\":pred_mean,\n","                    \"Std_dev\":std,\n","                    \"Var\":var}\n","      \n","      df_tosave = pd.DataFrame(dict_tosave)\n","      dfs.append(df_tosave)\n","      \n","      avg_RMSE.append(math.sqrt(mean_squared_error(y_true, pred_mean)))\n","      avg_bias.append(abs(statistics.mean(pred_mean)-statistics.mean(y_true)))\n","      \n","      \n","    a = pred[0,::,::,::,::]\n","    x1 = a[np.newaxis,::,::,::,::]\n","\n","  # Saving predictions (Note to change the datetime and also the required predictions to be saved)\n","  pred_start = datetime.datetime(2020,10,i_date,1,30,0)\n","  pred_end = datetime.datetime(2020,10,i_date+3,22,30,0)\n","  k = np.shape(dfs)[0]-1\n","  \n","  # Saving from last prediction\n","  while pred_end >= pred_start:\n","    pred_d = pred_end.strftime(\"%Y%m%d%H%M%S\")\n","    dfs[k].to_csv(path_OP+pred_d+\".csv\",index=False)\n","    k = k-1\n","    pred_end -= delta\n","\n","  RMSE = RMSE+avg_RMSE[-8:]\n","  bias = bias+avg_bias[-8:]\n","\n","  print(\"ONE DAY DONE!\")\n","\n","# The RMSE and bias are stored as \"Statistics.csv\" file \n","stat_dict = {\"RMSE\": RMSE,\n","             \"Bias\": bias}\n","stat_df = pd.DataFrame(stat_dict)\n","stat_df.to_csv(path_OP+\"Statistics.csv\",index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYA25q8bYWgB","outputId":"a0b90ee5-3f2f-4b39-a626-32459302f6ec","executionInfo":{"status":"ok","timestamp":1666068884134,"user_tz":420,"elapsed":2087778,"user":{"displayName":"Archana Kannan","userId":"16237662099459153058"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of frames_PPT is (27, 1624, 3856, 1)\n","shape of frames_SM is (27, 1624, 3856, 1)\n","Shape of test_frames_SM is (27, 1624, 3856, 1)\n","Shape of test_frames_PPT is (27, 1624, 3856, 1)\n","Shape of X_test_SM is (9, 3, 1624, 3856, 1)\n","Shape of X_test_PPT is (9, 3, 1624, 3856, 1)\n","Shape of y_act is (9, 3, 1624, 3856, 1)\n","1/1 [==============================] - 1s 937ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 76ms/step\n","1/1 [==============================] - 0s 72ms/step\n","1/1 [==============================] - 0s 84ms/step\n","1/1 [==============================] - 0s 88ms/step\n","1/1 [==============================] - 0s 77ms/step\n","1/1 [==============================] - 0s 71ms/step\n","1/1 [==============================] - 0s 83ms/step\n","1/1 [==============================] - 0s 76ms/step\n","1/1 [==============================] - 0s 80ms/step\n","1/1 [==============================] - 0s 73ms/step\n","1/1 [==============================] - 0s 82ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 77ms/step\n","1/1 [==============================] - 0s 80ms/step\n","1/1 [==============================] - 0s 78ms/step\n","1/1 [==============================] - 0s 80ms/step\n","1/1 [==============================] - 0s 76ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 82ms/step\n","1/1 [==============================] - 0s 79ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 80ms/step\n","1/1 [==============================] - 0s 82ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 83ms/step\n","1/1 [==============================] - 0s 84ms/step\n","1/1 [==============================] - 0s 78ms/step\n","1/1 [==============================] - 0s 77ms/step\n","1/1 [==============================] - 0s 77ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 88ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 81ms/step\n","1/1 [==============================] - 0s 80ms/step\n","1/1 [==============================] - 0s 86ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 76ms/step\n","1/1 [==============================] - 0s 99ms/step\n","1/1 [==============================] - 0s 79ms/step\n","ONE DAY DONE!\n","Shape of frames_PPT is (27, 1624, 3856, 1)\n","shape of frames_SM is (27, 1624, 3856, 1)\n","Shape of test_frames_SM is (27, 1624, 3856, 1)\n","Shape of test_frames_PPT is (27, 1624, 3856, 1)\n","Shape of X_test_SM is (9, 3, 1624, 3856, 1)\n","Shape of X_test_PPT is (9, 3, 1624, 3856, 1)\n","Shape of y_act is (9, 3, 1624, 3856, 1)\n","1/1 [==============================] - 1s 952ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 89ms/step\n","1/1 [==============================] - 0s 77ms/step\n","1/1 [==============================] - 0s 85ms/step\n","1/1 [==============================] - 0s 92ms/step\n","1/1 [==============================] - 0s 79ms/step\n","1/1 [==============================] - 0s 82ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 76ms/step\n","1/1 [==============================] - 0s 85ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 80ms/step\n","1/1 [==============================] - 0s 88ms/step\n","1/1 [==============================] - 0s 76ms/step\n","1/1 [==============================] - 0s 88ms/step\n","1/1 [==============================] - 0s 83ms/step\n","1/1 [==============================] - 0s 85ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 82ms/step\n","1/1 [==============================] - 0s 79ms/step\n","1/1 [==============================] - 0s 79ms/step\n","1/1 [==============================] - 0s 78ms/step\n","1/1 [==============================] - 0s 79ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 89ms/step\n","1/1 [==============================] - 0s 78ms/step\n","1/1 [==============================] - 0s 75ms/step\n","1/1 [==============================] - 0s 93ms/step\n","1/1 [==============================] - 0s 80ms/step\n","1/1 [==============================] - 0s 90ms/step\n","1/1 [==============================] - 0s 79ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 85ms/step\n","1/1 [==============================] - 0s 84ms/step\n","1/1 [==============================] - 0s 80ms/step\n","1/1 [==============================] - 0s 87ms/step\n","1/1 [==============================] - 0s 87ms/step\n","1/1 [==============================] - 0s 73ms/step\n","1/1 [==============================] - 0s 80ms/step\n","1/1 [==============================] - 0s 76ms/step\n","1/1 [==============================] - 0s 84ms/step\n","1/1 [==============================] - 0s 76ms/step\n","1/1 [==============================] - 0s 83ms/step\n","1/1 [==============================] - 0s 83ms/step\n","1/1 [==============================] - 0s 81ms/step\n","1/1 [==============================] - 0s 78ms/step\n","1/1 [==============================] - 0s 84ms/step\n","ONE DAY DONE!\n"]}]},{"cell_type":"code","source":["# No assimilation run\n","\n","# Paths for data and outputs\n","path_PPT = '/content/gdrive/My Drive/Archana/Colab_Notebooks/Data/SMAP_PPT_global/'\n","path_SM = '/content/gdrive/My Drive/Archana/Colab_Notebooks/Data/SMAP_SM_global/'\n","path_OP = \"/content/gdrive/My Drive/Archana/Colab_Notebooks/Outputs/Uncertainty_reduction_experiment_4_Oct/Pred_0/\"\n","\n","RMSE = []\n","bias = []\n","\n","# For loop for predicting every day for three days into the future using the last three(16:30,19:30:22:30) data\n","for i_date in range(1,3):\n","  start = datetime.datetime(2020,10,i_date,16,30,0) # START DATETIME\n","  end = datetime.datetime(2020,10,i_date+3,22,30,0) # END DATETIME\n","  delta = datetime.timedelta(hours=3) # DELTA T. For SMAP L4 DELTA=3hours\n","\n","  frames_PPT = [] \n","  frames_SM = []\n","\n","  while start <= end:\n","    d = start.strftime(\"%Y%m%d%H%M%S\")\n","    # Importing SMAP precipitation data\n","    img_PPT = Image.open(path_PPT+d+\"_ppt.jpeg\")\n","    f_PPT = img_PPT.convert(mode='RGB')\n","    f_PPT = np.array(f_PPT)\n","    f_PPT = np.sum(f_PPT, axis=2)\n","    f_PPT = np.array(f_PPT)\n","    f_PPT = f_PPT.reshape(f_PPT.shape[0], f_PPT.shape[1], 1)\n","    frames_PPT.append(f_PPT)\n","    \n","    # Importing SMAP soil moisture data\n","    img_SM = Image.open(path_SM+d+\"_sm.jpeg\")\n","    f_SM = img_SM.convert(mode='RGB')\n","    f_SM = np.array(f_SM)\n","    f_SM = np.sum(f_SM, axis=2)\n","    f_SM = np.array(f_SM)\n","    f_SM = f_SM.reshape(f_SM.shape[0], f_SM.shape[1], 1)\n","    frames_SM.append(f_SM)\n","\n","    start += delta\n","\n","  frames_PPT = np.array(frames_PPT)\n","  frames_SM = np.array(frames_SM)\n","  print(f\"Shape of frames_PPT is {np.shape(frames_PPT)}\")\n","  print(f\"shape of frames_SM is {np.shape(frames_SM)}\")\n","\n","  # Normalization of the SM and PPT images\n","\n","  test_frames_SM = frames_SM[0:(len(frames_SM)),:,:,:]\n","  test_frames_PPT = frames_PPT[0:(len(frames_PPT)),:,:,:]\n","\n","  maxi_SM = np.max(test_frames_SM)\n","  mini_SM = np.min(test_frames_SM)\n","\n","  maxi_PPT = np.max(test_frames_PPT)\n","  mini_PPT = np.min(test_frames_PPT)\n","\n","  test_frames_SM = (test_frames_SM-mini_SM)/(maxi_SM-mini_SM)\n","  test_frames_PPT = (test_frames_PPT-mini_PPT)/(maxi_PPT-mini_PPT)\n","\n","  print(f\"Shape of test_frames_SM is {np.shape(test_frames_SM)}\")\n","  print(f\"Shape of test_frames_PPT is {np.shape(test_frames_PPT)}\")\n","\n","  window = 3 # Interval, using 6 hours interval(i.e., three data prediction)\n","\n","  # Appending 3 images to a single variable ---> input to the model data structure\n","\n","  X_test_SM, X_test_PPT, y_act = [], [], []\n","\n","  for i in range(0, len(test_frames_PPT), 3):\n","    X_test_SM.append(test_frames_SM[i:i+window,::,::,::])\n","    X_test_PPT.append(test_frames_PPT[i:i+window,::,::,::])\n","\n","  for i in range(0, len(test_frames_SM), 3):\n","    y_act.append(test_frames_SM[i:i+window,::,::,::])\n","    \n","  X_test_SM = np.array(X_test_SM)\n","  X_test_PPT = np.array(X_test_PPT)\n","  y_act = np.array(y_act)\n","\n","  print(f\"Shape of X_test_SM is {np.shape(X_test_SM)}\")\n","  print(f\"Shape of X_test_PPT is {np.shape(X_test_PPT)}\")\n","  print(f\"Shape of y_act is {np.shape(y_act)}\")\n","\n","  # Loading trained model\n","\n","  model = tf.keras.models.load_model('/content/gdrive/My Drive/Archana/Colab_Notebooks/Models/global_model_v1.h5')\n","\n","  # Prediction\n","\n","  avg_RMSE = []\n","  avg_bias = []\n","  \n","  # a and b are just dummy variables, since np.newaxis did not work in a single line\n","  # xi's are the inputs, other inputs can be added\n","\n","  a = X_test_SM[0,::,::,::,::] \n","  x1 = a[np.newaxis,::,::,::,::] # x1 is the input - SM\n","\n","  dfs = []\n","\n","  # Predicting for three days\n","  for i in range(1,9):\n","    b = X_test_PPT[i-1,::,::,::,::]\n","    x2 = b[np.newaxis,::,::,::,::] # x2 is the input - PPT\n","    \n","    # Predicting in three hour intervals, model trained that way\n","    for j in range(0,3):\n","      predictions = []\n","      n_iter = 2 # Number of iterations for prediction, using dropout layer to predict the mean and other statistic\n","      for t in range(n_iter):\n","        pred = model.predict([x1, x2])\n","        y_pred = pred[::,j,::,::,::].flatten()\n","        predictions.append(y_pred)\n","      \n","      prediction_df = pd.DataFrame()\n","      pred_array = np.array(predictions)\n","      prediction_df['mean'] = pred_array.mean(axis=0).reshape(-1,)\n","      prediction_df['std'] = pred_array.std(axis=0).reshape(-1,)\n","      prediction_df['var'] = pred_array.var(axis=0).reshape(-1,)\n","\n","      y_true = y_act[i,j,::,::,::].flatten()\n","\n","      pred_mean = prediction_df['mean']\n","      std = prediction_df['std']\n","      var = prediction_df['var']\n","\n","\n","      dict_tosave = {\"y_pred\":pred_mean,\n","                    \"Std_dev\":std,\n","                    \"Var\":var}\n","      \n","      df_tosave = pd.DataFrame(dict_tosave)\n","      dfs.append(df_tosave)\n","      \n","      \n","      \n","      avg_RMSE.append(math.sqrt(mean_squared_error(y_true, pred_mean)))\n","      avg_bias.append(abs(statistics.mean(pred_mean)-statistics.mean(y_true)))\n","      \n","      \n","    a = pred[0,::,::,::,::]\n","    x1 = a[np.newaxis,::,::,::,::]\n","\n","  # Saving 3rd day predictions(in real time today)\n","  pred_start = datetime.datetime(2020,10,i_date+3,1,30,0)\n","  pred_end = datetime.datetime(2020,10,i_date+3,22,30,0)\n","  k = np.shape(dfs)[0]-1\n","  \n","  while pred_end >= pred_start:\n","    pred_d = pred_end.strftime(\"%Y%m%d%H%M%S\")\n","    dfs[k].to_csv(path_OP+pred_d+\".csv\",index=False)\n","    k = k-1\n","    pred_end -= delta\n","\n","  RMSE = RMSE+avg_RMSE[-8:]\n","  bias = bias+avg_bias[-8:]\n","\n","  print(\"ONE DAY DONE!\")\n","\n","stat_dict = {\"RMSE\": RMSE,\n","             \"Bias\": bias}\n","stat_df = pd.DataFrame(stat_dict)\n","stat_df.to_csv(path_OP+\"Statistics.csv\",index=False)"],"metadata":{"id":"4cNls3U4YXPr"},"execution_count":null,"outputs":[]}]}