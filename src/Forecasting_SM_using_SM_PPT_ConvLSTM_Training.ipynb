{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNW4i/n+6ydK5/m/5H8U08P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"siRpbqJrWvb2"},"outputs":[],"source":["# Imports\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import numpy as np\n","from PIL import Image\n","import datetime\n","import random\n","import tensorflow as tf\n","from pathlib import Path\n","\n","%load_ext tensorboard\n","!rm -rf ./logs/\n","\n","\n","path_SM = '/content/gdrive/My Drive/Archana/Colab_Notebooks/Data/SMAP_SM_global/'\n","path_PPT = '/content/gdrive/My Drive/Archana/Colab_Notebooks/Data/SMAP_PPT_global/'\n","\n","for i in range(1,32):\n","\n","\n","  start = datetime.datetime(2017,12,i,1,30,0) # START DATETIME\n","  end = datetime.datetime(2017,12,i,22,30,0) # END DATETIME\n","  delta = datetime.timedelta(hours=3) # DELTA T. For SMAP L4 DELTA=3hours\n","\n","  frames_SM = []\n","  frames_PPT = []\n","\n","  while start <= end:\n","    d = start.strftime(\"%Y%m%d%H%M%S\")\n","\n","    img_SM = Image.open(path_SM+d+\"_sm.jpeg\")\n","    img_PPT = Image.open(path_PPT+d+\"_ppt.jpeg\")\n","\n","    img_SM.show()\n","    img_PPT.show()\n","\n","    # print(np.shape(img_SM))\n","    # print(np.shape(img_PPT))\n","\n","    f_SM = img_SM.convert(mode='RGB')\n","    f_PPT = img_PPT.convert(mode='RGB')\n","\n","    f_SM = np.array(f_SM)\n","    f_PPT = np.array(f_PPT)\n","\n","    f_SM = np.sum(f_SM, axis=2)\n","    f_PPT = np.sum(f_PPT, axis=2)\n","\n","    f_SM = np.array(f_SM)\n","    f_PPT = np.array(f_PPT)\n","\n","    f_SM = f_SM.reshape(f_SM.shape[0], f_SM.shape[1], 1)\n","    f_PPT = f_PPT.reshape(f_PPT.shape[0], f_PPT.shape[1], 1)\n","\n","    frames_SM.append(f_SM)\n","    frames_PPT.append(f_PPT)\n","\n","    start += delta\n","\n","  frames_SM = np.array(frames_SM)\n","  frames_PPT = np.array(frames_PPT)\n","\n","  print(np.shape(frames_SM))\n","  print(np.shape(frames_PPT))\n","\n","  window = 3 # window is the interval of prediction. \n","  # Eg. window = 3 ==> three inputs, three outputs. Using 6hr inputs(3 i/ps) the next 6hr(3 o/ps) is predicted\n","\n","  # Train data\n","\n","  train_frames_SM = frames_SM[0:(len(frames_SM)),:,:,:]\n","  train_frames_PPT = frames_PPT[0:(len(frames_PPT)),:,:,:]\n","\n","  print(f\"Shape of train_frames_SM is {np.shape(train_frames_SM)}\")\n","  print(f\"Shape of train_frames_PPT is {np.shape(train_frames_PPT)}\")\n","\n","  # Normalization\n","\n","  maxi_SM = np.max(train_frames_SM)\n","  mini_SM = np.min(train_frames_SM)\n","  train_frames_SM = (train_frames_SM-mini_SM)/(maxi_SM-mini_SM)\n","\n","  maxi_PPT = np.max(train_frames_PPT)\n","  mini_PPT = np.min(train_frames_PPT)\n","  train_frames_PPT = (train_frames_PPT-mini_PPT)/(maxi_PPT-mini_PPT)\n","\n","  X_train_SM, X_train_PPT, y_train = [], [], []\n","\n","  for i in range(0, len(train_frames_SM)-2*window, 1):\n","    X_train_SM.append(train_frames_SM[i:i+window,::,::,::])\n","    X_train_PPT.append(train_frames_PPT[i:i+window,::,::,::])\n","    y_train.append(train_frames_SM[(i+window):(i+window+window),::,::,::])\n","\n","  X_train_SM = np.array(X_train_SM)\n","  X_train_PPT = np.array(X_train_PPT)  \n","  y_train = np.array(y_train) \n","\n","  print(f\"Shape of X_train_SM is {np.shape(X_train_SM)}\")\n","  print(f\"Shape of X_train_PPT is {np.shape(X_train_PPT)}\")\n","  print(f\"Shape of y_train is {np.shape(y_train)}\")\n","\n","  # Validation data\n","\n","  r=random.sample(range(len(train_frames_SM)-2*window), 1)\n","\n","  X_val_SM = X_train_SM[r, ::, ::, ::, ::]\n","  X_val_PPT = X_train_PPT[r, ::, ::, ::, ::]\n","\n","  y_val = y_train[r, ::, ::, ::, ::]\n","\n","  X_train_SM = np.delete(X_train_SM, (r), axis=0)\n","  X_train_PPT = np.delete(X_train_PPT, (r), axis=0)\n","\n","  y_train = np.delete(y_train, (r), axis=0) \n","\n","  # ConvLSTM model for SM\n","  inp_SM = tf.keras.layers.Input((window, 1624, 3856, 1))\n","  x1   =  tf.keras.layers.ConvLSTM2D(filters=2, kernel_size=(3, 3), padding='same', return_sequences=True) (inp_SM)\n","  x1   =  tf.keras.layers.ConvLSTM2D(filters=2, kernel_size=(3, 3), padding='same', return_sequences=True )(x1)\n","  m1  =  tf.keras.models.Model(inp_SM, x1)\n","\n","  # ConvLSTM model for PPT\n","  inp_PPT = tf.keras.layers.Input((window, 1624, 3856, 1))\n","  x2   =  tf.keras.layers.ConvLSTM2D(filters=2, kernel_size=(3, 3), padding='same', return_sequences=True) (inp_PPT)\n","  x2   =  tf.keras.layers.ConvLSTM2D(filters=2, kernel_size=(3, 3), padding='same', return_sequences=True)(x2)\n","  m2  =  tf.keras.models.Model(inp_PPT, x2)\n","\n","  m1.summary()\n","  m2.summary()\n","\n","  # Combined model\n","  combinedInput = tf.keras.layers.concatenate([m1.output, m2.output])\n","\n","  x = tf.keras.layers.ConvLSTM2D(filters=2, kernel_size=(3, 3), padding='same', return_sequences=True)(combinedInput)\n","  x = tf.keras.layers.Dropout(0.2)(x, training=True)\n","  x = tf.keras.layers.Conv3D(filters=1, kernel_size=(3, 3, 3), activation='sigmoid', padding='same')(x)\n","\n","  model = tf.keras.models.Model(inputs=[m1.input, m2.input], outputs=x)\n","\n","  model.summary()\n","\n","  if Path('/content/gdrive/My Drive/Archana/Colab_Notebooks/Models/global_model_v1.h5').is_file():\n","    print(\"Partially trained model found! Retraining an old model\")\n","    train_model = tf.keras.models.load_model('/content/gdrive/My Drive/Archana/Colab_Notebooks/Models/global_model_v1.h5')\n","  else:\n","    print(\"New model is being trained\")\n","    train_model = model\n","\n","\n","  best_model_file = 'vgg.h5'\n","  best_model = tf.keras.callbacks.ModelCheckpoint(best_model_file, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n","  train_model.compile(optimizer='adam', loss=\"mse\")\n","\n","  log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","  my_callbacks = [best_model, tensorboard_callback]\n","\n","  history = train_model.fit(x=[X_train_SM, X_train_PPT], y=y_train, validation_data=([X_val_SM, X_val_PPT], y_val), epochs=5, callbacks=my_callbacks,batch_size=20)\n","  train_model.load_weights(best_model_file)\n","  tf.keras.models.save_model(train_model,'/content/gdrive/My Drive/Archana/Colab_Notebooks/Models/global_model_v1.h5')\n"]}]}